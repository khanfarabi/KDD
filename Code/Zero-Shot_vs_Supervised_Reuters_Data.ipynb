{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqmfgytJsbjd"
      },
      "source": [
        "# Package Installation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uhpGu1KFowEP"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "def inst():\n",
        "        !pip install -U sentence-transformers\n",
        "        !pip install sentence_transformers\n",
        "        !pip install transformers\n",
        "        !pip install datasets\n",
        "        !pip install stop_words\n",
        "        #!pip install flair \n",
        "        !pip install scipy \n",
        "        !pip install https://github.com/scikit-learn-contrib/scikit-learn-extra/archive/master.zip\n",
        "inst()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uo8sZPh1swD2"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "kt8LG6oD8KGX",
        "outputId": "077b887c-cbbd-4b22-ec95-fc2485338988"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Reusing dataset reuters21578 (/root/.cache/huggingface/datasets/reuters21578/ModApte/1.0.0/98a2ad6a0242627562db83992f9625261854c40a88619322596153a5a16a206c)\n"
          ]
        }
      ],
      "source": [
        "# Data Preprocessing to retieve the texts with the multiple labels\n",
        "%%capture\n",
        "from datasets import load_dataset\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import operator\n",
        "import matplotlib.pyplot as plt\n",
        "#from transformers import pipeline\n",
        "from pylab import rcParams\n",
        "import sys \n",
        "import nltk\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "import csv\n",
        "import pandas as pd\n",
        "import sys\n",
        "samelocation=defaultdict(list)\n",
        "\n",
        "reuters= load_dataset('reuters21578', 'ModApte')\n",
        "\n",
        "random.seed(1)\n",
        "\n",
        "def clean(label):\n",
        "    label = re.sub(\"([a-z])([A-Z])\", \"\\\\1 \\\\2\", label)\n",
        "    label = label.replace(\"_\", \" \")\n",
        "    return label\n",
        "\n",
        "\n",
        "def sample_test_data(texts,  labels, title, size):\n",
        "    data = list(zip(texts, labels,title))\n",
        "    data = [item for item in data if len(item[0]) > 0]\n",
        "    random.shuffle(data)\n",
        "    texts, labels,title = zip(*data)\n",
        "    return texts[:size], labels[:size], title[:size],texts[size:], labels[size:]\n",
        "reuters_train_texts, reuters_train_labels,title, _, _ = sample_test_data(reuters['train'], reuters['train']['topics'],reuters['train']['places'], 50000)\n",
        "#reuters_train_texts, reuters_train_labels,labels, _ = sample_test_data(reuters['train'], reuters['train']['topics'],reuters['train']['places'], 50000)\n",
        "#labels,_ = sample_test_data( reuters['train']['places'], 50000)\n",
        "txt_place={}\n",
        "place_tx=[]\n",
        "for t in title:\n",
        "    for kk in t:\n",
        "        place_tx.append(kk)\n",
        "same_tp=[]\n",
        "for hh in reuters_train_labels:\n",
        "    for kk in hh:\n",
        "       same_tp.append(kk)\n",
        "sm_topic=defaultdict(list)\n",
        "txt=[]\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "import nltk\n",
        "from stop_words import get_stop_words\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = list(get_stop_words('en'))         #Have around 900 stopwords\n",
        "nltk_words = list(stopwords.words('english'))   #Have around 150 stopwords\n",
        "stop_words.extend(nltk_words)\n",
        "rttext_tag={}\n",
        "label_class=[]\n",
        "WORDS_ev={}\n",
        "sintag={}\n",
        "eid_maps={}\n",
        "eid_mapm={}\n",
        "rid=0\n",
        "rd_txt={}\n",
        "txt_rd={}\n",
        "#\n",
        "rttext_tags={}\n",
        "label_classs=[]\n",
        "WORDS_evs={}\n",
        "sintags={}\n",
        "eid_mapss={}\n",
        "eid_mapms={}\n",
        "rids=0\n",
        "rd_txts={}\n",
        "txt_rds={}\n",
        "\n",
        "for hh in range(len(reuters_train_labels)):\n",
        "  if len(reuters_train_labels[hh])>0 and len(reuters_train_texts[hh]['text'].split())>0:\n",
        "                  zz=reuters_train_labels[hh]\n",
        "                  #print(title[hh])\n",
        "                  #sys.exit()\n",
        "                  #print(\"text\"+\"\\n\")\n",
        "                  if len(zz)>1 and len(zz)<=3:\n",
        "                          vcc=0\n",
        "                          jj=re.sub('\\s+(a|an|and|the)(\\s+)', '\\2',reuters_train_texts[hh]['text'])\n",
        "                          from nltk.tokenize import word_tokenize\n",
        "                          tokens = word_tokenize(jj)\n",
        "                          # convert to lower case\n",
        "                          tokens = [w.lower() for w in tokens]\n",
        "                          # remove punctuation from each word\n",
        "                          import string\n",
        "                          table = str.maketrans('', '', string.punctuation)\n",
        "                          stripped = [w.translate(table) for w in tokens]\n",
        "                          # remove remaining tokens that are not alphabetic\n",
        "                          words = [word for word in stripped if word.isalpha()]\n",
        "                          # filter out stop words\n",
        "                          from nltk.corpus import stopwords\n",
        "                          stop_words = set(stopwords.words('english'))\n",
        "                          words = [w for w in words if not w in stop_words and len(w)>2]\n",
        "                          wrd=[]\n",
        "                          sz=''\n",
        "                          for k in words:\n",
        "                             # if vcc<150:\n",
        "                                wrd.append(k)\n",
        "                                vcc=vcc+1\n",
        "                          #rttext_tag[wrd]=zz[0]\n",
        "                          if len(wrd)>=2:\n",
        "                              WORDS_ev[rid]=wrd\n",
        "                              #rid=rid+1\n",
        "                              for vv in wrd:\n",
        "                                if vv!='reuter':\n",
        "                                    sz=sz+vv+\" \"\n",
        "                            #print(zz,sz)\n",
        "                              for kk in zz:\n",
        "                                if kk not in label_class:\n",
        "                                    label_class.append(kk)\n",
        "                              rttext_tag[sz]=zz\n",
        "                              txt_place[sz]=title[hh]\n",
        "                              samelocation[place_tx[hh]].append(sz)\n",
        "                              sm_topic[same_tp[hh]].append(rid)\n",
        "                              sintag[sz]=zz[0:1]\n",
        "                              eid_maps[rid]=zz[0:1]\n",
        "                              eid_mapm[rid]=zz\n",
        "                              rd_txt[rid]=sz\n",
        "                              txt_rd[sz]=rid\n",
        "                              rid=rid+1\n",
        "                          #print(reuters_train_texts[hh]['text'].split())\n",
        "                          #txt.append(reuters_train_texts[hh]['text'])\n",
        "                          #print(\"\\n\\n\")\n",
        "                  elif len(zz)==1:\n",
        "                                        vcc=0\n",
        "                                        jj=re.sub('\\s+(a|an|and|the)(\\s+)', '\\2',reuters_train_texts[hh]['text'])\n",
        "                                        from nltk.tokenize import word_tokenize\n",
        "                                        tokens = word_tokenize(jj)\n",
        "                                        # convert to lower case\n",
        "                                        tokens = [w.lower() for w in tokens]\n",
        "                                        # remove punctuation from each word\n",
        "                                        import string\n",
        "                                        table = str.maketrans('', '', string.punctuation)\n",
        "                                        stripped = [w.translate(table) for w in tokens]\n",
        "                                        # remove remaining tokens that are not alphabetic\n",
        "                                        words = [word for word in stripped if word.isalpha()]\n",
        "                                        # filter out stop words\n",
        "                                        from nltk.corpus import stopwords\n",
        "                                        stop_words = set(stopwords.words('english'))\n",
        "                                        words = [w for w in words if not w in stop_words and len(w)>=4]\n",
        "                                        wrd=[]\n",
        "                                        sz=''\n",
        "                                        for k in words:\n",
        "                                            if vcc<150:\n",
        "                                                wrd.append(k)\n",
        "                                                vcc=vcc+1\n",
        "                                        #rttext_tag[wrd]=zz[0]\n",
        "                                        if len(wrd)>=5:\n",
        "                                            WORDS_evs[rid]=wrd\n",
        "                                            #rid=rid+1\n",
        "                                            for vv in wrd:\n",
        "                                                if vv!='reuter':\n",
        "                                                    sz=sz+vv+\" \"\n",
        "                                            #print(zz,sz)\n",
        "                                            for kk in zz:\n",
        "                                                if kk not in label_classs:\n",
        "                                                    label_classs.append(kk)\n",
        "                                            rttext_tags[sz]=zz\n",
        "                                            sintags[sz]=zz#[0:1]\n",
        "                                            eid_mapss[rid]=zz#[0:1]\n",
        "                                            eid_mapms[rid]=zz\n",
        "                                            rd_txts[rid]=sz\n",
        "                                            txt_rds[sz]=rid\n",
        "                                            rids=rids+1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for tt in txt_place:\n",
        "    pass#print\n",
        "rttext_tag2=rttext_tag\n",
        "#rttext_tagh1=rttext_tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zEAm1LbNRqDa"
      },
      "outputs": [],
      "source": [
        "# Unique Tags: Getting the number of unique classes\n",
        "untr=[]\n",
        "str1=[]\n",
        "for vv in rttext_tag2:\n",
        "    for kk in rttext_tag2[vv]:\n",
        "        if kk not in str1:\n",
        "            str1.append(kk)\n",
        "s=set(str1)\n",
        "for zz in s:\n",
        "    untr.append(zz)\n",
        "#Checking Unique Tags\n",
        "#print(len(untr))\n",
        "for vv in untr:\n",
        "    pass#print(vv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcSDiIhFF314",
        "outputId": "d4c0a68d-cd8e-4f5b-ba0f-f30aa8178354"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "wheat 150\n",
            "grain 280\n",
            "interest 137\n",
            "crude 120\n",
            "money-fx 246\n",
            "corn 104\n",
            "['money-fx', 'wheat', 'crude', 'grain', 'corn']\n",
            "661\n"
          ]
        }
      ],
      "source": [
        "# Count the number of texts per tag and new assignment withe frequent tag class\n",
        "rttext_tags3={}\n",
        "untr1=[]\n",
        "def zsn(rttext_tag2,tags):\n",
        "    cc=0\n",
        "    fg=[]\n",
        "    for tt in rttext_tag2:\n",
        "        if tags in rttext_tag2[tt]:\n",
        "            cc=cc+1\n",
        "    return tags,cc\n",
        "\n",
        "ptg={}\n",
        "for tt in untr:\n",
        "    tags,cc=zsn(rttext_tag2,tt)\n",
        "    if cc>100:\n",
        "        untr1.append(tags)\n",
        "        ptg[tags]=cc\n",
        "\n",
        "\n",
        "print(len(ptg))\n",
        "for vv in ptg:\n",
        "    #if ptg[vv]>=100:\n",
        "         print(vv,ptg[vv])\n",
        "\n",
        "for vc in rttext_tag2:\n",
        "       vb=[]\n",
        "       for ty in ptg:\n",
        "           if ty in rttext_tag2[vc]:\n",
        "               vb.append(ty)\n",
        "               break\n",
        "           else:\n",
        "                continue\n",
        "       if len(vb)>0:\n",
        "          rttext_tags3[vc]=rttext_tag2[vc]\n",
        "for kk in rttext_tags3:\n",
        "      pass#print(rttext_tags3[kk])\n",
        "untr1=['money-fx', 'wheat', 'crude', 'grain', 'corn']\n",
        "print(untr1)\n",
        "print(len(rttext_tags3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbWoVn7yP3sI"
      },
      "outputs": [],
      "source": [
        " 'grain','money-fx','wheat','interest', and 'crude'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MviwpQ9N-z7S"
      },
      "source": [
        "# Performing Inference for the zero-shot and the supervised models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vWTqUFxEvgC"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import numpy as np\n",
        "import operator\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import pipeline\n",
        "from pylab import rcParams\n",
        "import sys \n",
        "import nltk\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import wordnet as wn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import model_selection, naive_bayes, svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "def zsn(rttext_tags3,tags):\n",
        "        cc=0\n",
        "        cc1=0\n",
        "        vz='non'+str(tags)\n",
        "        Train_X=[] \n",
        "        Test_X=[]\n",
        "        Train_Y=[]\n",
        "        Test_Y=[]\n",
        "        # = model_selection.train_test_split(dt,lb,test_size=0.5)\n",
        "        ct1=0\n",
        "        ct2=100\n",
        "\n",
        "        cs1=0\n",
        "        cs2=100\n",
        "        rttext_tagh1={}\n",
        "\n",
        "        for tt in rttext_tags3:\n",
        "            gh=[]\n",
        "            gh1=[]\n",
        "            for kk in rttext_tags3[tt]:\n",
        "                if tags in kk:\n",
        "                    #print(kk)\n",
        "                    #cc=cc+1\n",
        "                    if tt not in rttext_tagh1:\n",
        "                        if cc<50:\n",
        "                            gh.append(tags)\n",
        "                            if len(rttext_tagh1)<=100:\n",
        "                                rttext_tagh1[tt]=gh\n",
        "                                cc=cc+1\n",
        "                            #break\n",
        "                elif tags not in kk:\n",
        "                    if tt not in rttext_tagh1:\n",
        "                        if cc1<50:\n",
        "                            gh1.append(vz)\n",
        "                            if len(rttext_tagh1)<=100:\n",
        "                                rttext_tagh1[tt]=gh1\n",
        "                                cc1=cc1+1\n",
        "                            #break\n",
        "        lbu=[tags,vz]\n",
        "        #if tags=='wheat':\n",
        "             #print(len(rttext_tagh1))\n",
        "             #for kk in rttext_tagh1:\n",
        "                    #print(rttext_tagh1[kk])\n",
        "        #sys.exit()\n",
        "\n",
        "\n",
        "        zero_shot_classifier = pipeline(\"zero-shot-classification\")\n",
        "        def review_explain(text):\n",
        "                result = zero_shot_classifier(sequences =text,candidate_labels =lbu ,multi_label=False)\n",
        "                \n",
        "                return result['labels'],result['scores']\n",
        "\n",
        "\n",
        "        txt_lbp={}\n",
        "        for jj in rttext_tagh1:\n",
        "            cl,sc=review_explain(jj)\n",
        "            ghh=[]\n",
        "            ghh.append(cl[0])\n",
        "            txt_lbp[jj]=ghh\n",
        "        #F1 Score\n",
        "        pd=[]\n",
        "        tr=[]\n",
        "        for tt in txt_lbp:\n",
        "            if tt in rttext_tagh1:\n",
        "                pd.append(str(txt_lbp[tt][0]))\n",
        "                tr.append(str(rttext_tagh1[tt][0]))\n",
        "        from sklearn.metrics import f1_score\n",
        "        #print(f1_score(tr, pd, average='micro'))\n",
        "        zrp=f1_score(tr, pd, average='micro')\n",
        "        print(\" Zero-shot prediction for the Class: \"+str(tags)+\"\\n\")\n",
        "        print(zrp)\n",
        "\n",
        "        '''\n",
        "        #F1 Score\n",
        "        pd=[]\n",
        "        tr=[]\n",
        "        for tt in txt_lbp:\n",
        "            if tt in rttext_tagh1:\n",
        "                pd.append(str(txt_lbp[tt][0]))\n",
        "                tr.append(str(rttext_tagh1[tt][0]))\n",
        "        from sklearn.metrics import f1_score\n",
        "        print(f1_score(tr, pd, average='micro'))\n",
        "\n",
        "        # data and label train test\n",
        "        Train_X=[] \n",
        "        Test_X=[]\n",
        "        Train_Y=[]\n",
        "        Test_Y=[]\n",
        "        # = model_selection.train_test_split(dt,lb,test_size=0.5)\n",
        "        ct1=0\n",
        "        ct2=100\n",
        "\n",
        "        cs1=0\n",
        "        cs2=100\n",
        "        dt=[]\n",
        "        lb=[]\n",
        "        for hh in rttext_tagh1:\n",
        "            gg=hh.split()\n",
        "            for nn in rttext_tagh1[hh]:\n",
        "                for kk in gg:\n",
        "                    if kk not in dt:\n",
        "                        dt.append(kk)\n",
        "                        lb.append(nn)\n",
        "        print(len(dt),len(lb))\n",
        "        l1=len(dt)//2\n",
        "        l2=len(lb)//2\n",
        "        for bb in range(0,l1):\n",
        "            if lb[bb]=='gas':\n",
        "                Train_X.append(dt[bb])\n",
        "                Train_Y.append(lb[bb])\n",
        "        for bb in range(l1,len(dt)):\n",
        "            if lb[bb]=='gas':\n",
        "                Test_X.append(dt[bb])\n",
        "                Test_Y.append(lb[bb])\n",
        "        for bb in range(0,l1):\n",
        "            if lb[bb]=='acq':\n",
        "                Train_X.append(dt[bb])\n",
        "                Train_Y.append(lb[bb])\n",
        "        for bb in range(l1,len(dt)):\n",
        "            if lb[bb]=='acq':\n",
        "                Test_X.append(dt[bb])\n",
        "                Test_Y.append(lb[bb])\n",
        "\n",
        "        print(len(Train_X),len(Train_Y),len(Test_X),len(Test_Y))\n",
        "        '''\n",
        "        # data and label train test\n",
        "       # Train_X=[] \n",
        "        #Test_X=[]\n",
        "        #Train_Y=[]\n",
        "        #Test_Y=[]\n",
        "        # = model_selection.train_test_split(dt,lb,test_size=0.5)\n",
        "        ct1=0\n",
        "        ct2=100\n",
        "\n",
        "        cs1=0\n",
        "        cs2=100\n",
        "        dt=[]\n",
        "        lb=[]\n",
        "        for hh in rttext_tagh1:\n",
        "            gg=hh.split()\n",
        "            for nn in rttext_tagh1[hh]:\n",
        "                for kk in gg:\n",
        "                    if kk not in dt:\n",
        "                        dt.append(kk)\n",
        "                        lb.append(nn)\n",
        "        print(len(dt),len(lb))\n",
        "        l1=len(dt)//2\n",
        "        l2=len(lb)//2\n",
        "        for bb in range(0,l1):\n",
        "            if lb[bb]==tags:\n",
        "                pass#Train_X.append(dt[bb])\n",
        "                pass#Train_Y.append(tags)\n",
        "        for bb in range(l1,len(dt)):\n",
        "            if lb[bb]==tags:\n",
        "                pass#Test_X.append(dt[bb])\n",
        "                pass#Test_Y.append(tags)\n",
        "        for bb in range(0,l1):\n",
        "            if vz in lb[bb]:\n",
        "                pass#Train_X.append(dt[bb])\n",
        "                pass#Train_Y.append(vz)\n",
        "        for bb in range(l1,len(dt)):\n",
        "            if vz in lb[bb]:\n",
        "                pass#Test_X.append(dt[bb])\n",
        "                pass#Test_Y.append(vz)\n",
        "\n",
        "       # print(len(Train_X),len(Train_Y),len(Test_X),len(Test_Y))\n",
        "        Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(dt,lb,test_size=0.5)\n",
        "        #import sys\n",
        "       # print\n",
        "\n",
        "\n",
        "\n",
        "        Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
        "        #Tfidf_vect.fit(Train_X)\n",
        "        f = Tfidf_vect.fit_transform(dt)\n",
        "        Train_X_Tfidf = Tfidf_vect.fit_transform(Train_X)\n",
        "        Test_X_Tfidf = Tfidf_vect.transform(Test_X)\n",
        "        # Classifier - Algorithm - SVM\n",
        "        # fit the training dataset on the classifier\n",
        "        SVM = svm.SVC(C=50.0, kernel='linear') #svm.LinearSVC(C=1)#\n",
        "        SVM.fit(Train_X_Tfidf,Train_Y)\n",
        "        # predict the labels on validation dataset\n",
        "        predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
        "        # Use accuracy_score function to get the accuracy\n",
        "        #print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
        "        from sklearn.metrics import f1_score\n",
        "        #print(f1_score(Test_Y, predictions_SVM, average='weighted'))\n",
        "        svmp=f1_score(Test_Y, predictions_SVM, average='micro')\n",
        "        print(\" SVM prediction for the Class: \"+str(tags)+\"\\n\")\n",
        "        print(svmp)\n",
        "\n",
        "        ## naive bayes\n",
        "        Naive = naive_bayes.MultinomialNB()\n",
        "        Naive.fit(Train_X_Tfidf,Train_Y)\n",
        "        # predict the labels on validation dataset\n",
        "        predictions_NB = Naive.predict(Test_X_Tfidf)\n",
        "        # Use accuracy_score function to get the accuracy\n",
        "        #print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, Test_Y)*100)\n",
        "        from sklearn.metrics import f1_score\n",
        "        #print(f1_score(Test_Y, predictions_NB, average='micro'))\n",
        "        nvp=f1_score(Test_Y, predictions_NB, average='micro')\n",
        "        print(\" Naive Bayes prediction for the Class: \"+str(tags)+\"\\n\")\n",
        "        print(nvp)\n",
        "        # Random fores\n",
        "        from sklearn.ensemble import RandomForestClassifier\n",
        "        clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
        "        clf.fit(Train_X_Tfidf,Train_Y)\n",
        "        predictions_rf = clf.predict(Test_X_Tfidf)\n",
        "        # Use accuracy_score function to get the accuracy\n",
        "        #print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, Test_Y)*100)\n",
        "        from sklearn.metrics import f1_score\n",
        "        #print(f1_score(Test_Y, predictions_NB, average='micro'))\n",
        "        rfp=f1_score(Test_Y, predictions_rf, average='micro')\n",
        "        print(\" Random Forest prediction for the Class: \"+str(tags)+\"\\n\")\n",
        "        print(rfp)\n",
        "        # Bagging\n",
        "        from sklearn.ensemble import BaggingClassifier\n",
        "        from sklearn.svm import SVC\n",
        "        clf1=BaggingClassifier(base_estimator=SVC(),n_estimators=10, random_state=0)\n",
        "        clf1.fit(Train_X_Tfidf,Train_Y)\n",
        "        predictions_bg = clf1.predict(Test_X_Tfidf)\n",
        "        # Use accuracy_score function to get the accuracy\n",
        "        #print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, Test_Y)*100)\n",
        "        from sklearn.metrics import f1_score\n",
        "        #print(f1_score(Test_Y, predictions_NB, average='micro'))\n",
        "        bgp=f1_score(Test_Y, predictions_bg, average='micro')\n",
        "        print(\" Bagging prediction for the Class: \"+str(tags)+\"\\n\")\n",
        "        print(bgp)\n",
        "        # Decision Trees\n",
        "        from sklearn.tree import DecisionTreeClassifier\n",
        "        clf2 = DecisionTreeClassifier(random_state=0)\n",
        "        clf2.fit(Train_X_Tfidf,Train_Y)\n",
        "        predictions_dc = clf2.predict(Test_X_Tfidf)\n",
        "        # Use accuracy_score function to get the accuracy\n",
        "        #print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, Test_Y)*100)\n",
        "        from sklearn.metrics import f1_score\n",
        "        #print(f1_score(Test_Y, predictions_NB, average='micro'))\n",
        "        dcp=f1_score(Test_Y, predictions_dc, average='micro')\n",
        "        print(\" Decision Tree prediction for the Class: \"+str(tags)+\"\\n\")\n",
        "        print(dcp)\n",
        "        # KNN\n",
        "        from sklearn.neighbors import KNeighborsClassifier\n",
        "        clf3 = KNeighborsClassifier(n_neighbors=5)\n",
        "        clf3.fit(Train_X_Tfidf,Train_Y)\n",
        "        predictions_kn = clf3.predict(Test_X_Tfidf)\n",
        "        # Use accuracy_score function to get the accuracy\n",
        "        #print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, Test_Y)*100)\n",
        "        from sklearn.metrics import f1_score\n",
        "        #print(f1_score(Test_Y, predictions_NB, average='micro'))\n",
        "        knp=f1_score(Test_Y, predictions_kn, average='micro')\n",
        "        print(\" KNN prediction for the Class: \"+str(tags)+\"\\n\")\n",
        "        print(knp)\n",
        "        return zrp,svmp,nvp,rfp,bgp,dcp,knp\n",
        "zsn_p={}\n",
        "for tt in untr1:\n",
        "    try:\n",
        "        ggh=[]\n",
        "        zrp,svmp,nvp,rfp,bgp,dcp,knp =zsn(rttext_tags3,tt)\n",
        "        ggh.append(zrp)\n",
        "        ggh.append(svmp)\n",
        "        ggh.append(nvp)\n",
        "        ggh.append(rfp)\n",
        "        #ggh.append(bgp)\n",
        "        ggh.append(dcp)\n",
        "        #ggh.append(knp)\n",
        "        if len(ggh)>1:\n",
        "            zsn_p[tt]=ggh\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NE8HdHMp_CA1"
      },
      "source": [
        "#Computing Average F-1 score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKCzM10FUpnR"
      },
      "outputs": [],
      "source": [
        "# average f1 scores \n",
        "rtz_zs=[]\n",
        "def f1avg(nn):\n",
        "        s=0\n",
        "        for tt in zsn_p:\n",
        "            for vv in range(0,len(zsn_p[tt])):\n",
        "                if vv==nn:\n",
        "                    s=s+zsn_p[tt][vv]\n",
        "                    #print(zsn_p[tt][vv])\n",
        "        cv=s/len(zsn_p)\n",
        "        return cv\n",
        "for nn in range(0,5):\n",
        "    sc=f1avg(nn)\n",
        "    rtz_zs.append(sc)\n",
        "print(rtz_zs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hy-v9tb__HC8"
      },
      "source": [
        "# The Average F-1 scores per dominant classes for the zero-shot and the supervised models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PXtjPqIlkUy"
      },
      "outputs": [],
      "source": [
        "for tt in zsn_p:\n",
        "    print(tt,zsn_p[tt])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ry82NYdgKWbN"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2Jaxn7EQnxY"
      },
      "outputs": [],
      "source": [
        "## Testing the Stability of the models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELHBqVBFAMGP"
      },
      "outputs": [],
      "source": [
        "#Statistocs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DedhfchQurH"
      },
      "source": [
        "## Computing the standard deviation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLXzcLhEAG_U"
      },
      "outputs": [],
      "source": [
        "wheat=[0.83, 0.7335140018066847, 0.7335140018066847, 0.7335140018066847, 0.7335140018066847]\n",
        "#interest=[0.51, 0.6588139723801787, 0.6588139723801787, 0.6588139723801787, 0.6588139723801787]\n",
        "money_fx=[0.65, 0.6121164437450826, 0.6121164437450826, 0.6121164437450826, 0.6121164437450826]\n",
        "corn=[0.7699999999999999, 0.6697872340425531, 0.6697872340425531, 0.6697872340425531, 0.6697872340425531]\n",
        "crude=[0.68, 0.52990851513019, 0.52990851513019, 0.52990851513019, 0.52990851513019]\n",
        "grain=[0.82, 0.6061806656101426, 0.6061806656101426, 0.6061806656101426, 0.6061806656101426]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_9bwWuGwQzgi"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import numpy as np\n",
        "import operator\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import pipeline\n",
        "from pylab import rcParams\n",
        "import sys \n",
        "import nltk\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import wordnet as wn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import model_selection, naive_bayes, svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "def run1():\n",
        "                def zsn(rttext_tags3,tags):\n",
        "                        cc=0\n",
        "                        cc1=0\n",
        "                        vz='non'+str(tags)\n",
        "                        Train_X=[] \n",
        "                        Test_X=[]\n",
        "                        Train_Y=[]\n",
        "                        Test_Y=[]\n",
        "                        # = model_selection.train_test_split(dt,lb,test_size=0.5)\n",
        "                        ct1=0\n",
        "                        ct2=100\n",
        "\n",
        "                        cs1=0\n",
        "                        cs2=100\n",
        "                        rttext_tagh1={}\n",
        "\n",
        "                        for tt in rttext_tags3:\n",
        "                            gh=[]\n",
        "                            gh1=[]\n",
        "                            for kk in rttext_tags3[tt]:\n",
        "                                if tags in kk:\n",
        "                                    #print(kk)\n",
        "                                    #cc=cc+1\n",
        "                                    if tt not in rttext_tagh1:\n",
        "                                        if cc<100:\n",
        "                                            gh.append(tags)\n",
        "                                            if len(rttext_tagh1)<=200:\n",
        "                                                rttext_tagh1[tt]=gh\n",
        "                                                cc=cc+1\n",
        "                                            #break\n",
        "                                elif tags not in kk:\n",
        "                                    if tt not in rttext_tagh1:\n",
        "                                        if cc1<100:\n",
        "                                            gh1.append(vz)\n",
        "                                            if len(rttext_tagh1)<=200:\n",
        "                                                rttext_tagh1[tt]=gh1\n",
        "                                                cc1=cc1+1\n",
        "                                            #break\n",
        "                        lbu=[tags,vz]\n",
        "                        #if tags=='wheat':\n",
        "                            #print(len(rttext_tagh1))\n",
        "                            #for kk in rttext_tagh1:\n",
        "                                    #print(rttext_tagh1[kk])\n",
        "                        #sys.exit()\n",
        "\n",
        "\n",
        "                        zero_shot_classifier = pipeline(\"zero-shot-classification\")\n",
        "                        def review_explain(text):\n",
        "                                result = zero_shot_classifier(sequences =text,candidate_labels =lbu ,multi_label=False)\n",
        "                                \n",
        "                                return result['labels'],result['scores']\n",
        "\n",
        "\n",
        "                        txt_lbp={}\n",
        "                        for jj in rttext_tagh1:\n",
        "                            cl,sc=review_explain(jj)\n",
        "                            ghh=[]\n",
        "                            ghh.append(cl[0])\n",
        "                            txt_lbp[jj]=ghh\n",
        "                        #F1 Score\n",
        "                        pd=[]\n",
        "                        tr=[]\n",
        "                        for tt in txt_lbp:\n",
        "                            if tt in rttext_tagh1:\n",
        "                                pd.append(str(txt_lbp[tt][0]))\n",
        "                                tr.append(str(rttext_tagh1[tt][0]))\n",
        "                        from sklearn.metrics import f1_score\n",
        "                        #print(f1_score(tr, pd, average='micro'))\n",
        "                        zrp=f1_score(tr, pd, average='micro')\n",
        "                        print(\" Zero-shot prediction for the Class: \"+str(tags)+\"\\n\")\n",
        "                        print(zrp)\n",
        "\n",
        "                        '''\n",
        "                        #F1 Score\n",
        "                        pd=[]\n",
        "                        tr=[]\n",
        "                        for tt in txt_lbp:\n",
        "                            if tt in rttext_tagh1:\n",
        "                                pd.append(str(txt_lbp[tt][0]))\n",
        "                                tr.append(str(rttext_tagh1[tt][0]))\n",
        "                        from sklearn.metrics import f1_score\n",
        "                        print(f1_score(tr, pd, average='micro'))\n",
        "\n",
        "                        # data and label train test\n",
        "                        Train_X=[] \n",
        "                        Test_X=[]\n",
        "                        Train_Y=[]\n",
        "                        Test_Y=[]\n",
        "                        # = model_selection.train_test_split(dt,lb,test_size=0.5)\n",
        "                        ct1=0\n",
        "                        ct2=100\n",
        "\n",
        "                        cs1=0\n",
        "                        cs2=100\n",
        "                        dt=[]\n",
        "                        lb=[]\n",
        "                        for hh in rttext_tagh1:\n",
        "                            gg=hh.split()\n",
        "                            for nn in rttext_tagh1[hh]:\n",
        "                                for kk in gg:\n",
        "                                    if kk not in dt:\n",
        "                                        dt.append(kk)\n",
        "                                        lb.append(nn)\n",
        "                        print(len(dt),len(lb))\n",
        "                        l1=len(dt)//2\n",
        "                        l2=len(lb)//2\n",
        "                        for bb in range(0,l1):\n",
        "                            if lb[bb]=='gas':\n",
        "                                Train_X.append(dt[bb])\n",
        "                                Train_Y.append(lb[bb])\n",
        "                        for bb in range(l1,len(dt)):\n",
        "                            if lb[bb]=='gas':\n",
        "                                Test_X.append(dt[bb])\n",
        "                                Test_Y.append(lb[bb])\n",
        "                        for bb in range(0,l1):\n",
        "                            if lb[bb]=='acq':\n",
        "                                Train_X.append(dt[bb])\n",
        "                                Train_Y.append(lb[bb])\n",
        "                        for bb in range(l1,len(dt)):\n",
        "                            if lb[bb]=='acq':\n",
        "                                Test_X.append(dt[bb])\n",
        "                                Test_Y.append(lb[bb])\n",
        "\n",
        "                        print(len(Train_X),len(Train_Y),len(Test_X),len(Test_Y))\n",
        "                        '''\n",
        "                        # data and label train test\n",
        "                    # Train_X=[] \n",
        "                        #Test_X=[]\n",
        "                        #Train_Y=[]\n",
        "                        #Test_Y=[]\n",
        "                        # = model_selection.train_test_split(dt,lb,test_size=0.5)\n",
        "                        ct1=0\n",
        "                        ct2=100\n",
        "\n",
        "                        cs1=0\n",
        "                        cs2=100\n",
        "                        dt=[]\n",
        "                        lb=[]\n",
        "                        for hh in rttext_tagh1:\n",
        "                            gg=hh.split()\n",
        "                            for nn in rttext_tagh1[hh]:\n",
        "                                for kk in gg:\n",
        "                                    if kk not in dt:\n",
        "                                        dt.append(kk)\n",
        "                                        lb.append(nn)\n",
        "                        print(len(dt),len(lb))\n",
        "                        l1=len(dt)//2\n",
        "                        l2=len(lb)//2\n",
        "                        for bb in range(0,l1):\n",
        "                            if lb[bb]==tags:\n",
        "                                pass#Train_X.append(dt[bb])\n",
        "                                pass#Train_Y.append(tags)\n",
        "                        for bb in range(l1,len(dt)):\n",
        "                            if lb[bb]==tags:\n",
        "                                pass#Test_X.append(dt[bb])\n",
        "                                pass#Test_Y.append(tags)\n",
        "                        for bb in range(0,l1):\n",
        "                            if vz in lb[bb]:\n",
        "                                pass#Train_X.append(dt[bb])\n",
        "                                pass#Train_Y.append(vz)\n",
        "                        for bb in range(l1,len(dt)):\n",
        "                            if vz in lb[bb]:\n",
        "                                pass#Test_X.append(dt[bb])\n",
        "                                pass#Test_Y.append(vz)\n",
        "\n",
        "                    # print(len(Train_X),len(Train_Y),len(Test_X),len(Test_Y))\n",
        "                        Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(dt,lb,test_size=0.5)\n",
        "                        #import sys\n",
        "                    # print\n",
        "\n",
        "\n",
        "\n",
        "                        Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
        "                        #Tfidf_vect.fit(Train_X)\n",
        "                        f = Tfidf_vect.fit_transform(dt)\n",
        "                        Train_X_Tfidf = Tfidf_vect.fit_transform(Train_X)\n",
        "                        Test_X_Tfidf = Tfidf_vect.transform(Test_X)\n",
        "                        # Classifier - Algorithm - SVM\n",
        "                        # fit the training dataset on the classifier\n",
        "                        SVM = svm.SVC(C=50.0, kernel='linear') #svm.LinearSVC(C=1)#\n",
        "                        SVM.fit(Train_X_Tfidf,Train_Y)\n",
        "                        # predict the labels on validation dataset\n",
        "                        predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
        "                        # Use accuracy_score function to get the accuracy\n",
        "                        #print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
        "                        from sklearn.metrics import f1_score\n",
        "                        #print(f1_score(Test_Y, predictions_SVM, average='weighted'))\n",
        "                        svmp=f1_score(Test_Y, predictions_SVM, average='micro')\n",
        "                        print(\" SVM prediction for the Class: \"+str(tags)+\"\\n\")\n",
        "                        print(svmp)\n",
        "\n",
        "                        ## naive bayes\n",
        "                        Naive = naive_bayes.MultinomialNB()\n",
        "                        Naive.fit(Train_X_Tfidf,Train_Y)\n",
        "                        # predict the labels on validation dataset\n",
        "                        predictions_NB = Naive.predict(Test_X_Tfidf)\n",
        "                        # Use accuracy_score function to get the accuracy\n",
        "                        #print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, Test_Y)*100)\n",
        "                        from sklearn.metrics import f1_score\n",
        "                        #print(f1_score(Test_Y, predictions_NB, average='micro'))\n",
        "                        nvp=f1_score(Test_Y, predictions_NB, average='micro')\n",
        "                        print(\" Naive Bayes prediction for the Class: \"+str(tags)+\"\\n\")\n",
        "                        print(nvp)\n",
        "                        # Random fores\n",
        "                        from sklearn.ensemble import RandomForestClassifier\n",
        "                        clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
        "                        clf.fit(Train_X_Tfidf,Train_Y)\n",
        "                        predictions_rf = clf.predict(Test_X_Tfidf)\n",
        "                        # Use accuracy_score function to get the accuracy\n",
        "                        #print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, Test_Y)*100)\n",
        "                        from sklearn.metrics import f1_score\n",
        "                        #print(f1_score(Test_Y, predictions_NB, average='micro'))\n",
        "                        rfp=f1_score(Test_Y, predictions_rf, average='micro')\n",
        "                        print(\" Random Forest prediction for the Class: \"+str(tags)+\"\\n\")\n",
        "                        print(rfp)\n",
        "                        # Bagging\n",
        "                        from sklearn.ensemble import BaggingClassifier\n",
        "                        from sklearn.svm import SVC\n",
        "                        clf1=BaggingClassifier(base_estimator=SVC(),n_estimators=10, random_state=0)\n",
        "                        clf1.fit(Train_X_Tfidf,Train_Y)\n",
        "                        predictions_bg = clf1.predict(Test_X_Tfidf)\n",
        "                        # Use accuracy_score function to get the accuracy\n",
        "                        #print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, Test_Y)*100)\n",
        "                        from sklearn.metrics import f1_score\n",
        "                        #print(f1_score(Test_Y, predictions_NB, average='micro'))\n",
        "                        bgp=f1_score(Test_Y, predictions_bg, average='micro')\n",
        "                        print(\" Bagging prediction for the Class: \"+str(tags)+\"\\n\")\n",
        "                        print(bgp)\n",
        "                        # Decision Trees\n",
        "                        from sklearn.tree import DecisionTreeClassifier\n",
        "                        clf2 = DecisionTreeClassifier(random_state=0)\n",
        "                        clf2.fit(Train_X_Tfidf,Train_Y)\n",
        "                        predictions_dc = clf2.predict(Test_X_Tfidf)\n",
        "                        # Use accuracy_score function to get the accuracy\n",
        "                        #print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, Test_Y)*100)\n",
        "                        from sklearn.metrics import f1_score\n",
        "                        #print(f1_score(Test_Y, predictions_NB, average='micro'))\n",
        "                        dcp=f1_score(Test_Y, predictions_dc, average='micro')\n",
        "                        print(\" Decision Tree prediction for the Class: \"+str(tags)+\"\\n\")\n",
        "                        print(dcp)\n",
        "                        # KNN\n",
        "                        from sklearn.neighbors import KNeighborsClassifier\n",
        "                        clf3 = KNeighborsClassifier(n_neighbors=5)\n",
        "                        clf3.fit(Train_X_Tfidf,Train_Y)\n",
        "                        predictions_kn = clf3.predict(Test_X_Tfidf)\n",
        "                        # Use accuracy_score function to get the accuracy\n",
        "                        #print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, Test_Y)*100)\n",
        "                        from sklearn.metrics import f1_score\n",
        "                        #print(f1_score(Test_Y, predictions_NB, average='micro'))\n",
        "                        knp=f1_score(Test_Y, predictions_kn, average='micro')\n",
        "                        print(\" KNN prediction for the Class: \"+str(tags)+\"\\n\")\n",
        "                        print(knp)\n",
        "                        return zrp,svmp,nvp,rfp,bgp,dcp,knp\n",
        "                zsn_p={}\n",
        "                for tt in untr1:\n",
        "                    try:\n",
        "                        ggh=[]\n",
        "                        zrp,svmp,nvp,rfp,bgp,dcp,knp =zsn(rttext_tags3,tt)\n",
        "                        ggh.append(zrp)\n",
        "                        ggh.append(svmp)\n",
        "                        ggh.append(nvp)\n",
        "                        ggh.append(rfp)\n",
        "                        #ggh.append(bgp)\n",
        "                        ggh.append(dcp)\n",
        "                        #ggh.append(knp)\n",
        "                        if len(ggh)>1:\n",
        "                            zsn_p[tt]=ggh\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "                # average f1 scores \n",
        "                rtz_zs=[]\n",
        "                tb={}\n",
        "                def f1avg(nn):\n",
        "                        s=0\n",
        "                        tb1=[]\n",
        "                        for tt in zsn_p:\n",
        "                            for vv in range(0,len(zsn_p[tt])):\n",
        "                                if vv==nn:\n",
        "                                    s=s+zsn_p[tt][vv]\n",
        "                                    tb1.append(float(zsn_p[tt][vv]))\n",
        "                                    #print(zsn_p[tt][vv])\n",
        "                        cv=s/len(zsn_p)\n",
        "                        return cv,tb1\n",
        "                for nn in range(0,5):\n",
        "                    sc,tb1=f1avg(nn)\n",
        "                    rtz_zs.append(sc)\n",
        "                    tb[nn]=tb1\n",
        "                print(rtz_zs)\n",
        "                return rtz_zs,tb\n",
        "rtz_zs1={}\n",
        "rtz_zs2={}\n",
        "for kk in range(0,10):\n",
        "    rtz_zs,tb=run1()\n",
        "    rtz_zs1[kk]=rtz_zs\n",
        "    rtz_zs2[kk]=tb\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_tZxewFBhThD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics\n",
        "zr=[]\n",
        "zrr=[]\n",
        "svm=[]\n",
        "svmr=[]\n",
        "nb=[]\n",
        "nbr=[]\n",
        "rf=[]\n",
        "rfr=[]\n",
        "dt=[]\n",
        "dtr=[]\n",
        "for kk in rtz_zs2:\n",
        "   # print(kk)\n",
        "        for tt in rtz_zs2[kk]:\n",
        "            #print(tt,statistics.mean(rtz_zs2[kk][tt]),statistics.stdev(rtz_zs2[kk][tt]))\n",
        "            if tt==0:\n",
        "                zr.append(statistics.mean(rtz_zs2[kk][tt]))\n",
        "                zrr.append(statistics.stdev(rtz_zs2[kk][tt]))\n",
        "            elif tt==1:\n",
        "                svm.append(statistics.mean(rtz_zs2[kk][tt]))\n",
        "                svmr.append(statistics.stdev(rtz_zs2[kk][tt]))\n",
        "            elif tt==2:\n",
        "                nb.append(statistics.mean(rtz_zs2[kk][tt]))\n",
        "                nbr.append(statistics.stdev(rtz_zs2[kk][tt]))\n",
        "            elif tt==3:\n",
        "                rf.append(statistics.mean(rtz_zs2[kk][tt]))\n",
        "                rfr.append(statistics.stdev(rtz_zs2[kk][tt]))\n",
        "            elif tt==4:\n",
        "                dt.append(statistics.mean(rtz_zs2[kk][tt]))\n",
        "                dtr.append(statistics.stdev(rtz_zs2[kk][tt]))\n",
        "\n",
        "\n",
        "   # print(\"\\n\\n\")\n",
        "print(max(zrr))\n",
        "print(max(svmr))\n",
        "print(statistics.mean(zr))\n",
        "print(statistics.mean(svm))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEzMa5p3XpGn",
        "outputId": "aa7f4e50-190d-4863-a1fa-edbcd876650d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.08543508332708218\n",
            "0.09434470642507009\n",
            "0.7671489361702127\n",
            "0.6542150067930897\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Varying the number run \n",
        "\n",
        "\n",
        "\n",
        "#figures [0.334, 0.312, 0.28400000000000003, 0.268, 0.218]\n",
        "#[0.0461519230368573, 0.084970583144992, 0.029664793948382655, 0.06140032573203501, 0.0363318042491699]\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.dates import date2num\n",
        "import sys\n",
        "import pylab \n",
        "x1 = np.linspace(0, 20, 1000)\n",
        "\n",
        "\n",
        "sw=[0.7075, 0.7224999999999999, 0.73, 0.7324999999999999, 0.7424999999999999]\n",
        "swv=[0.0050000000000000044, 0.0050000000000000044, 0.0, 0.0050000000000000044, 0.0050000000000000044]#[0.0017000000000000008, 0.0006300000000000011, 0.00043000000000000075, 0.00013000000000000023, 0.00013000000000000023] #[0.45,0.52,0.55,0.61,0.70]\n",
        "#sw1=[0.45,0.49,0.58,0.65,0.72] #0.15,0.29,\n",
        "#srm1=[0.54,0.65,0.71,0.78,0.92]\n",
        "src1=[0.725, 0.7375, 0.76, 0.775, 0.7825]\n",
        "src1v=[0.005773502691896263, 0.0050000000000000044, 0.008164965809277268, 0.012909944487358068, 0.0050000000000000044]#[0.0002700000000000005, 0.00037000000000000065, 0.0004800000000000009, 0.0002200000000000004, 0.00025000000000000044]\n",
        "rc1=[0.3125, 0.3125, 0.32, 0.3225, 0.3175]\n",
        "rc1v=[0.0050000000000000044, 0.00957427107756339, 0.014142135623730963, 0.017078251276599347, 0.00957427107756339]\n",
        "\n",
        "rc2=[0.5125, 0.5475000000000001, 0.55, 0.5475000000000001, 0.555]\n",
        "rc2v=[0.00957427107756339, 0.00957427107756339, 0.008164965809277268, 0.015000000000000013, 0.012909944487358025]\n",
        "#[0.33, 0.324, 0.29400000000000004, 0.378, 0.326]\n",
        "#[0.08031189202104505, 0.09555103348473003, 0.057706152185014035, 0.050199601592044535, 0.07162401831787994]\n",
        "#relation\n",
        "#[0.334, 0.312, 0.28400000000000003, 0.268, 0.218]\n",
        "#[0.0461519230368573, 0.084970583144992, 0.029664793948382655, 0.06140032573203501, 0.0363318042491699]\n",
        "x2=0\n",
        "y2=0\n",
        "#plt.title('Comparison of accuracy of  feed back and without feedback using  bagging model with respect to annotated data')\n",
        "\n",
        "#plt.grid(True)\n",
        "#Lc1=['SHAP','LIME','20-Cluster','40-Cluster','60-Cluster','80-Cluster','100-Cluster']\n",
        "#Lc1=['SHAP','LIME','25-Models','50-Models','100-Models','200-Models','300-Models']#['SVM','Extra_Trees','Bagging','RandomForest','Decision_Tress']\n",
        "#Lc1=['0.01','0.02','0.03','0.04','0.05']\n",
        "Lc1=['1','2','3','4','5','6','7','8','9','10']\n",
        "\n",
        "#plt.hist(L22,density=100, bins=200) \n",
        "#plt.axis([0,6,0,50]) \n",
        "#axis([xmin,xmax,ymin,ymax])\n",
        "#txt=\"Our Approach vs LIME for Spam\"\n",
        "\n",
        "# make some synthetic data\n",
        "\n",
        "\n",
        "#fig = plt.figure()\n",
        "#fig.text(.5, .015, txt, ha='center')\n",
        "#plt.xlabel('Q6,Q7 and Q8 ')\n",
        "#plt.xlabel('Reviews ')\n",
        "plt.ylabel(\"Average  F-1 Score\",fontsize=15)\n",
        "plt.xlabel(\"Number of Run\",fontsize=15)\n",
        "x = np.array([0,1,2,3,4,5,6,7,8,9])\n",
        "ax = plt.subplot(111)\n",
        "ax1 = plt.subplot(111)\n",
        "ax2 = plt.subplot(111)\n",
        "ax3 = plt.subplot(111)\n",
        "\n",
        "############\n",
        "v = np.array(zr)\n",
        "x = [1,2,3,4,5,6,7,8,9,10]\n",
        "yr = zrr\n",
        "ax.errorbar(x,v,yerr=yr,color='k')\n",
        "v1 = np.array(svm)\n",
        "x1 = [1,2,3,4,5,6,7,8,9,10]\n",
        "yr1 = svmr\n",
        "ax1.errorbar(x1,v1,yerr=yr1,color='0.5')\n",
        "'''\n",
        "v2 = np.array(nb)\n",
        "x2 = [1,2,3,4,5,6,7,8,9,10]\n",
        "yr2 = nbr\n",
        "ax1.errorbar(x2,v2,yerr=yr2,color='0.2')\n",
        "#########\n",
        "\n",
        "v3 = np.array(rc1)\n",
        "x3 = [1,2,3,4,5]\n",
        "yr3 = rc1v\n",
        "ax2.errorbar(x3,v3,yerr=yr3,color='b')\n",
        "v4 = np.array(rc2)\n",
        "x4 = [1,2,3,4,5]\n",
        "yr4 = rc2v\n",
        "ax3.errorbar(x4,v4,yerr=yr4,color='y')\n",
        "\n",
        "'''\n",
        "#######\n",
        "#plt.show()\n",
        "#ax.errorbar(x2,y2, e2, linestyle='None', marker='|',color='g')\n",
        "#ax1.errorbar(x1,y1,e1, linestyle='None', marker='|',color='c')\n",
        "#plt.axhline(y=0.16,linestyle='-',color='0.8', xmin=0.0)\n",
        "#plt.axhline(y=0.24,linestyle='-',color='0.2', xmin=0.0)\n",
        "#plt.axhline(y=0.40,linestyle='-',color='b', xmin=0.0)\n",
        "#plt.axhline(y=0.35,linestyle='-',color='y', xmin=0.0)\n",
        "#plt.axhline(y=0.37,linestyle='-',color='C3', xmin=0.0)\n",
        "#plt.axhline(y=0.63,linestyle='-',color='C1', xmin=0.0)\n",
        "\n",
        "#ax2.axhline(y= 0.15, color = 'rgb', linestyle = '-') \n",
        "#ax3.axhline(y = 0.29, color = 'bg', linestyle = '-') \n",
        "#ax.bar(x-0.30,k,width=0.15,color='g',align='center')\n",
        "#ax.bar(x-0.15,e,width=0.15,color='b',align='center')\n",
        "#ax.bar(x,r,width=0.15,color='m',align='center')\n",
        "#ax.bar(x+0.15,b,width=0.15,color='c',align='center')\n",
        "#ax.bar(x+0.60,cl20,width=0.30,color='y',align='center')\n",
        "#pylab.plot(x2, y2, '-r', label='SHAP')\n",
        "#pylab.plot(x2,y2, '-m', label='LIME')\n",
        "pylab.plot(x2,y2, 'k', label=' Zero-Shot')\n",
        "pylab.plot(x2,y2, '0.5', label='SVM')\n",
        "#pylab.plot(x2,y2, '0.2', label='NB')\n",
        "#pylab.plot(x2,y2, '-C1', label='M-Explain (Relation)')\n",
        "#pylab.plot(x2,y2, '-C3', label='M-Explain (Word)')\n",
        "#pylab.plot(x2,y2, '-g', label='I-Explain (Relation)')\n",
        "#pylab.plot(x2,y2, '-c', label='I-Explain (Word)')\n",
        "#pylab.plot(x2,y2, '-c', label='Bagging')\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.font_manager import FontProperties\n",
        "\n",
        "#fontP = FontProperties()\n",
        "#fontP.set_size('xx-small')\n",
        "#pylab.legend(bbox_to_anchor=(1.05, 1), loc='upper left', prop=fontP)\n",
        "pylab.legend(loc='lower left')\n",
        "#plt.bar(x,pre)\n",
        "#plt1.bar(x,re)\n",
        "plt.xticks(x,Lc1)\n",
        "ax.set_xticklabels(ax.get_xticklabels(), fontsize=7,fontweight='bold')\n",
        "plt.savefig(\"Reuters_Zero_SP.pdf\",bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "pylab.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "a29fUNZchU9j",
        "outputId": "c666d4a8-22c0-48a9-85c5-7a62ac1458f2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:57: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:58: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Zero_SVM_NB_RF_DT_Reuters_Data_CMPl.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}